"""
Script ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c s·ª≠ d·ª•ng ST-IoU (Spatio-Temporal IoU)
So s√°nh k·∫øt qu·∫£ d·ª± ƒëo√°n v·ªõi ground truth annotations

ST-IoU xem x√©t c·∫£ kh√¥ng gian (bounding box) v√† th·ªùi gian (frame) nh∆∞ m·ªôt kh·ªëi 3D
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Set
import numpy as np
from tqdm import tqdm


def calculate_spatial_iou(bbox1: Tuple[int, int, int, int], bbox2: Tuple[int, int, int, int]) -> float:
    """
    T√≠nh Spatial IoU gi·ªØa 2 bounding boxes

    Args:
        bbox1, bbox2: (x1, y1, x2, y2) coordinates

    Returns:
        IoU score (0 to 1)
    """
    x1_1, y1_1, x2_1, y2_1 = bbox1
    x1_2, y1_2, x2_2, y2_2 = bbox2

    # T√≠nh di·ªán t√≠ch giao
    inter_x1 = max(x1_1, x1_2)
    inter_y1 = max(y1_1, y1_2)
    inter_x2 = min(x2_1, x2_2)
    inter_y2 = min(y2_1, y2_2)

    if inter_x2 < inter_x1 or inter_y2 < inter_y1:
        return 0.0

    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)

    # T√≠nh di·ªán t√≠ch h·ª£p
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    union_area = area1 + area2 - inter_area

    if union_area == 0:
        return 0.0

    return inter_area / union_area


def calculate_temporal_iou(frames1: Set[int], frames2: Set[int]) -> float:
    """
    T√≠nh Temporal IoU gi·ªØa 2 t·∫≠p h·ª£p frames

    Args:
        frames1, frames2: T·∫≠p h·ª£p frame IDs

    Returns:
        Temporal IoU score (0 to 1)
    """
    intersection = len(frames1 & frames2)
    union = len(frames1 | frames2)

    if union == 0:
        return 0.0

    return intersection / union


def calculate_st_iou(
    pred_bboxes: List[Dict],
    gt_bboxes: List[Dict]
) -> float:
    """
    T√≠nh ST-IoU (Spatio-Temporal IoU) gi·ªØa predicted volume v√† ground truth volume

    Args:
        pred_bboxes: List of predicted bboxes v·ªõi 'frame' key
        gt_bboxes: List of ground truth bboxes v·ªõi 'frame' key

    Returns:
        ST-IoU score (0 to 1)
    """
    if not pred_bboxes or not gt_bboxes:
        return 0.0

    # T·∫°o t·∫≠p h·ª£p frames
    pred_frames = {bbox['frame'] for bbox in pred_bboxes}
    gt_frames = {bbox['frame'] for bbox in gt_bboxes}

    # Frames chung (intersection)
    common_frames = pred_frames & gt_frames

    if not common_frames:
        return 0.0

    # T√≠nh spatial overlap cho c√°c frames chung
    spatial_overlap_volume = 0.0
    total_volume = 0.0

    # T√≠nh t·ª´ predicted bboxes
    for pred_bbox in pred_bboxes:
        frame = pred_bbox['frame']
        if frame in gt_frames:
            # T√¨m bbox t∆∞∆°ng ·ª©ng ·ªü ground truth
            matching_gt_bboxes = [
                bbox for bbox in gt_bboxes
                if bbox['frame'] == frame
            ]

            if matching_gt_bboxes:
                # T√≠nh max spatial IoU v·ªõi b·∫•t k·ª≥ ground truth n√†o
                pred_bbox_coords = (pred_bbox['x1'], pred_bbox['y1'], pred_bbox['x2'], pred_bbox['y2'])
                max_spatial_iou = 0.0

                for gt_bbox in matching_gt_bboxes:
                    gt_bbox_coords = (gt_bbox['x1'], gt_bbox['y1'], gt_bbox['x2'], gt_bbox['y2'])
                    spatial_iou = calculate_spatial_iou(pred_bbox_coords, gt_bbox_coords)
                    max_spatial_iou = max(max_spatial_iou, spatial_iou)

                spatial_overlap_volume += max_spatial_iou
            total_volume += 1

    # T√≠nh t·ª´ ground truth bboxes (ƒë·ªÉ kh√¥ng miss)
    for gt_bbox in gt_bboxes:
        frame = gt_bbox['frame']
        if frame in pred_frames:
            matching_pred_bboxes = [
                bbox for bbox in pred_bboxes
                if bbox['frame'] == frame
            ]

            if not matching_pred_bboxes:
                total_volume += 1

    # ST-IoU = (spatial intersection volume) / (spatial union volume)
    temporal_iou = calculate_temporal_iou(pred_frames, gt_frames)

    # ST-IoU k·∫øt h·ª£p spatial v√† temporal
    if total_volume > 0:
        spatial_accuracy = spatial_overlap_volume / total_volume
    else:
        spatial_accuracy = 0.0

    st_iou = spatial_accuracy * temporal_iou

    return st_iou


def load_ground_truth(annotation_path: str) -> Dict:
    """
    Load ground truth annotations t·ª´ JSON file

    Args:
        annotation_path: ƒê∆∞·ªùng d·∫´n t·ªõi annotations.json

    Returns:
        Dictionary ch·ª©a ground truth data
    """
    with open(annotation_path, 'r') as f:
        annotations = json.load(f)

    gt_dict = {}
    for item in annotations:
        video_id = item['video_id']
        # M·ªói video c√≥ th·ªÉ c√≥ multiple objects (annotations)
        gt_dict[video_id] = item.get('annotations', [])

    return gt_dict


def load_predictions(results_dir: str) -> Dict:
    """
    Load predictions t·ª´ results directory

    Args:
        results_dir: ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£

    Returns:
        Dictionary ch·ª©a predictions {video_id: list of all bboxes}
    """
    predictions = {}

    if not os.path.exists(results_dir):
        print(f"‚ö†Ô∏è Results directory not found: {results_dir}")
        return predictions

    for filename in os.listdir(results_dir):
        if filename.endswith('_results.json') and not filename.startswith('evaluation'):
            filepath = os.path.join(results_dir, filename)
            try:
                with open(filepath, 'r') as f:
                    data = json.load(f)
                    video_id = data.get('video_id')
                    if video_id:
                        # Parse detections v√† flatten bboxes
                        all_bboxes = []
                        detections = data.get('detections', [])

                        # N·∫øu detections l√† danh s√°ch c√°c objects, m·ªói object c√≥ bboxes
                        if isinstance(detections, list):
                            for detection in detections:
                                if isinstance(detection, dict):
                                    # Format t·ª´ pipeline: {"bboxes": [...]}
                                    if 'bboxes' in detection:
                                        for bbox in detection.get('bboxes', []):
                                            all_bboxes.append(bbox)
                                    # Ho·∫∑c format kh√°c: detection l√† bbox tr·ª±c ti·∫øp
                                    elif 'frame' in detection:
                                        all_bboxes.append(detection)

                        predictions[video_id] = all_bboxes
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading {filename}: {e}")

    return predictions


def evaluate_video(
    video_id: str,
    predictions: List[Dict],
    ground_truth_annotations: List[Dict]
) -> Dict:
    """
    ƒê√°nh gi√° m·ªôt video s·ª≠ d·ª•ng ST-IoU

    Args:
        video_id: ID c·ªßa video
        predictions: Danh s√°ch d·ª± ƒëo√°n
        ground_truth_annotations: Danh s√°ch ground truth (c√≥ th·ªÉ nhi·ªÅu objects)

    Returns:
        Dictionary ch·ª©a metrics cho video n√†y
    """
    # N·∫øu kh√¥ng c√≥ predictions ho·∫∑c ground truth
    if not ground_truth_annotations:
        st_iou_scores = []
    elif not predictions:
        st_iou_scores = [0.0] * len(ground_truth_annotations)
    else:
        # T√≠nh ST-IoU v·ªõi m·ªói ground truth object
        st_iou_scores = []
        for gt_obj in ground_truth_annotations:
            gt_bboxes = gt_obj.get('bboxes', [])
            if gt_bboxes:
                st_iou = calculate_st_iou(predictions, gt_bboxes)
                st_iou_scores.append(st_iou)

    # L·∫•y mean ST-IoU cho video n√†y
    mean_st_iou = np.mean(st_iou_scores) if st_iou_scores else 0.0

    # T√≠nh th√™m c√°c metrics kh√°c
    num_gt_objects = len(ground_truth_annotations)
    num_predictions = len(predictions)

    # T√≠nh s·ªë frame ƒë∆∞·ª£c detect
    pred_frames = {p.get('frame') for p in predictions}
    all_gt_frames = set()
    for gt_obj in ground_truth_annotations:
        for bbox in gt_obj.get('bboxes', []):
            all_gt_frames.add(bbox['frame'])

    common_frames = len(pred_frames & all_gt_frames)
    total_gt_frames = len(all_gt_frames)
    frame_recall = common_frames / total_gt_frames if total_gt_frames > 0 else 0.0

    metrics = {
        'video_id': video_id,
        'st_iou': mean_st_iou,
        'per_object_st_iou': st_iou_scores,
        'num_gt_objects': num_gt_objects,
        'num_predictions': num_predictions,
        'num_gt_frames': total_gt_frames,
        'num_detected_frames': common_frames,
        'frame_recall': frame_recall,
    }

    return metrics


def evaluate_all(
    annotations_path: str,
    results_dir: str
) -> Dict:
    """
    ƒê√°nh gi√° to√†n b·ªô dataset s·ª≠ d·ª•ng ST-IoU

    Args:
        annotations_path: ƒê∆∞·ªùng d·∫´n t·ªõi annotations.json
        results_dir: ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c k·∫øt qu·∫£

    Returns:
        Dictionary ch·ª©a t·ªïng h·ª£p metrics
    """
    print(f"\nüìä Spatio-Temporal IoU (ST-IoU) Evaluation")
    print("=" * 80)

    # Load ground truth v√† predictions
    print(f"\nüìÇ Loading ground truth from {annotations_path}...")
    ground_truth = load_ground_truth(annotations_path)
    print(f"‚úÖ Loaded {len(ground_truth)} videos")

    print(f"\nüìÇ Loading predictions from {results_dir}...")
    predictions = load_predictions(results_dir)
    print(f"‚úÖ Loaded {len(predictions)} videos with predictions")

    # ƒê√°nh gi√° t·ª´ng video
    print(f"\nüîç Evaluating each video with ST-IoU...")
    all_metrics = []

    for video_id in tqdm(sorted(ground_truth.keys()), desc="Evaluating"):
        gt = ground_truth.get(video_id, [])
        preds = predictions.get(video_id, [])

        metrics = evaluate_video(
            video_id,
            preds,
            gt
        )
        all_metrics.append(metrics)

    # T√≠nh t·ªïng h·ª£p
    print("\n" + "=" * 80)
    print("üìà Overall Results (ST-IoU Metric)")
    print("=" * 80)

    st_iou_scores = [m['st_iou'] for m in all_metrics]
    mean_st_iou = np.mean(st_iou_scores)
    std_st_iou = np.std(st_iou_scores)
    min_st_iou = np.min(st_iou_scores)
    max_st_iou = np.max(st_iou_scores)

    total_gt_objects = sum(m['num_gt_objects'] for m in all_metrics)
    total_predictions = sum(m['num_predictions'] for m in all_metrics)
    total_gt_frames = sum(m['num_gt_frames'] for m in all_metrics)
    total_detected_frames = sum(m['num_detected_frames'] for m in all_metrics)

    print(f"\nüìä ST-IoU Statistics:")
    print(f"  Mean ST-IoU: {mean_st_iou:.4f}")
    print(f"  Std Dev: {std_st_iou:.4f}")
    print(f"  Min ST-IoU: {min_st_iou:.4f}")
    print(f"  Max ST-IoU: {max_st_iou:.4f}")

    print(f"\nüìà Detection Statistics:")
    print(f"  Total Ground Truth Objects: {total_gt_objects}")
    print(f"  Total Predictions: {total_predictions}")
    print(f"  Ground Truth Frames: {total_gt_frames}")
    print(f"  Detected Frames: {total_detected_frames}")

    frame_recall = total_detected_frames / total_gt_frames if total_gt_frames > 0 else 0.0
    print(f"  Frame Recall: {frame_recall:.4f} ({frame_recall*100:.2f}%)")

    # In chi ti·∫øt t·ª´ng video
    print(f"\nüìã Per-Video ST-IoU Results:")
    print("-" * 80)
    print(f"{'Video ID':<20} {'ST-IoU':<10} {'Objects':<10} {'Preds':<10} {'Frame Recall':<15}")
    print("-" * 80)

    for m in sorted(all_metrics, key=lambda x: x['st_iou'], reverse=True):
        print(f"{m['video_id']:<20} {m['st_iou']:<10.4f} {m['num_gt_objects']:<10} "
              f"{m['num_predictions']:<10} {m['frame_recall']:<15.4f}")

    print("-" * 80)
    print(f"{'AVERAGE':<20} {mean_st_iou:<10.4f}")

    # L∆∞u k·∫øt qu·∫£ chi ti·∫øt
    results_summary = {
        'metric_type': 'ST-IoU (Spatio-Temporal IoU)',
        'overall_metrics': {
            'mean_st_iou': float(mean_st_iou),
            'std_st_iou': float(std_st_iou),
            'min_st_iou': float(min_st_iou),
            'max_st_iou': float(max_st_iou),
            'frame_recall': float(frame_recall),
            'total_gt_objects': int(total_gt_objects),
            'total_predictions': int(total_predictions),
            'total_gt_frames': int(total_gt_frames),
            'total_detected_frames': int(total_detected_frames),
        },
        'per_video_metrics': all_metrics
    }

    return results_summary


def save_evaluation_results(summary: Dict, output_path: str) -> None:
    """
    L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o file JSON

    Args:
        summary: Dictionary ch·ª©a t·ªïng h·ª£p metrics
        output_path: ƒê∆∞·ªùng d·∫´n file output
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, 'w') as f:
        json.dump(summary, f, indent=2)

    print(f"\nüíæ Evaluation results saved to: {output_path}")


if __name__ == "__main__":
    import sys

    # C√°c ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh
    dataset_root = '/Users/huuthieu/Desktop/learning/zaloAI/train'
    annotations_path = os.path.join(dataset_root, 'annotations', 'annotations.json')
    results_dir = './results'

    # Cho ph√©p override t·ª´ command line
    if len(sys.argv) > 1:
        results_dir = sys.argv[1]
    if len(sys.argv) > 2:
        annotations_path = sys.argv[2]

    print(f"üöÅ Drone Object Detection - ST-IoU Evaluation")
    print("=" * 80)
    print(f"Annotations: {annotations_path}")
    print(f"Results: {results_dir}")

    # Ch·∫°y ƒë√°nh gi√°
    summary = evaluate_all(
        annotations_path=annotations_path,
        results_dir=results_dir
    )

    # L∆∞u k·∫øt qu·∫£
    output_path = os.path.join(results_dir, 'evaluation_results.json')
    save_evaluation_results(summary, output_path)

    print("\n‚úÖ Evaluation complete!")
